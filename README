This project is focused on decoding of audio captchas, which often might be much easier 
then breaking visual captchas.
It was tested on uloz.to audio captchas (as of August 2014) - also included are modified plugins for pyload 
- so no need to manually enter captchas for uloz.to site.

Code is hosted on https://code.launchpad.net/~ivan-zderadicka/adecaptcha/trunk

Some description (of older version) is here:   http://old.zderadicka.eu/adecaptcha.php

------------------------------------------------------------------------
How to make it work (on linux) - uloz.to capchas recognition with PyLoad:
------------------------------------------------------------------------

bzr checkout http://bazaar.launchpad.net/~ivan-zderadicka/adecaptcha/trunk/

cd trunk/

sudo apt-get install build-essential python-dev python-numpy python-scipy python-pymad python-pip
sudo pip install cython

#build cython extensions
python setup.py build_ext --inplace

cd adecaptcha/libsvm-3.17/

make clean
make lib

cd ..

#re-link 
rm svm;ln -s libsvm-3.17/python/ svm

# testing if it works - should print correct capcha text (get http:://sound url from ulozto site 
- initiate download of a file copy url under link "prehrat kod")
python adecaptcha.py ulozto.cfg http://sound_url 

cd ..
#moving to system python path
sudo mv  adecaptcha/ /usr/local/lib/python2.7/dist-packages/

#If you want to use with pyload copy plugins to your local plugin dir
 cp -av pyload/userplugins/   ~/.pyload/

# and restart pyLoadCore
-------------------------------------------------------------------------------

Advanced - how to train for new audio samples
-------------------------------------------------------------------------------
Download enough samples ~500 should be enough (if sample contains 4 letters),
load them in one directory sound and picture must have same base name (xyz.wav and xyz.gif)

Start python samplestool.py (requires also pyqt4, pyao, matplotlib modules). 
There set:
- Directory with samples -   to your downloaded samples
- Segment min.length (s) - minimum length of a letter sound - should be something like 0.2 - 0.3 or close
- Silence min. length (s) - this is minimum length of silence between two letters - should be 
something like 0.1 - depends
- Number of freq. bins - number of frequency spectrum bins to which signal is divided - this set size
of feature vector for each letter sound
- Threshold for segmentation - threshold to distinguish some sound from silence - value is 
 mean energy envelope  (basically sum of squares of amplitude with small window, 
divided by window size, scaled to values 0-100).  This would be very individual for samples -  you can 
click "Show Energy Envelope" - this will show chart of energy envelope for current sample -  try 
few samples and you will get idea what right value should be (segments are also added small offset before 
and after they reach the threshold)
- Check "Play sound when move to next item" - it'll help to work with samples

Then click Next  button and it opens first sample, go through several samples to set segmentation data 
correctly.

When you think segmentation is set properly, check "Analyze segmentation initially" - it then 
calculates number of segments for all samples - ideally all samples should have same number of samples.

If there is some initial/final sound, that is not part of captcha text, you can set what segments should 
be considered as a part of captcha  (supports python indexing - so end = -4 means last 4 segments)

Now you need to transcribe all samples - write letters corresponding for current captcha and click next. 
It's bit dull job but should not take less then hour for 500 samples.

If all samples are transcribed  you can generate training data for SVM classificator and save 
segmentation configuration. Click "Generate Training Data" button.  After clicking file 
dialog is opened - save data within 'libsm-3.17/tools' directory say as 'xxx.data'. Progress dialog appears 
so wait while  process ends.

Before creating  model for your data assure that you have packages gnuplot and gnuplot-x11 installed 
(they are used by training tools).
  
First we need to split data to training and test sets (test set will be used to measure 
classificator accuracy) - for test data we can dedicate about 10% of all data set, that's 200 in this case. 

cd libsm-3.17/tools
 ./subset.py xxx.data 200 xxx.test xxx.train
 
 now we can do training of the SVM classifier with our data:
./easy.py ulozto.train ulozto.test

this can take quite some time - half hour, so keep it running (you'll see some chart from gnuplot) 
See result - specially check of classification accuracy, in ideal case you should see:
Accuracy = 100% (200/200) (classification)

after this we have all we need, just copy
cp xxx.data.cfg ../../xxx.cfg
cp xxx.train.range ../../xxx.range
cp xxx.train.model ../../xxx.model

cd ../..

edit here xxx.cfg - add there keys to point to model and range files:
{ ... 
#add to end
, 'range_file':'xxx.range', 'model_file':'xxx.model' 
}


and can test if it works:
./adechaptcha.py xxx.conf "http://some_server/audio_captcha.wav"
 
-----------------------------------------------------------------------

Thanks to authors of all these great tools and libraries: numpy, scipy,  pymad, pyao, pyqt4, pyload, matplotlib 

History:
4-Apr-13 -  Fixed issues with libsvm seg fault when called from multiple threads
12-Aug-14 - updated with some fixes - support of wav,  new segmentation algorithm,  pyload plugin update, ...

